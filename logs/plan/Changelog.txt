## Changelog and thinking notes
- Added type hints and docstrings
- With examples, the LLM able to investigate array of strings/pointers further
- Agent is able to understand functionality of a function based on the usage of it (going up in call-graph) (with direct instruction).
- Switched to mcp architecture


#### Thoughts:
- Team agents might be needed to validate and evaluate results of the agent like naming, assumptions, 
    help the main agent get further and refine results.
- How would feedback agent look like?
- It seems that adding context about the binary **really help** the agent
    understand and name much better. What will be better,
    docs or short summary?
- Should I gather some of the context beforehand? (usage, decompiled_code, call_graph etc..)
- Agent doesn't validate it's assumptions, very bad.
- Deconstruct simple tasks? (like understanding a memory blob: ptr, data, etc ..)

#### Tasks:
1. ~~Add call graph flow~~
2. Test - Workflow with multiple prompts of techniques
    1. fix: list exports, list_imports, 
    2. reduce multiple functions for the same function (list_strings, search_strings, etc ..)
    3. inject get_memory_map
    4. test get_call_graph
    5. Prompt tuning
        1. Xref not analyzed properly
        2. Memory content need further analysis
3. Give the agent access to api docs (webtool/knowledgebase)
4. Create roadmap + parallel strategies to research and test.